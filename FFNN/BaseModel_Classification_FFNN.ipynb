{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539345d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as T\n",
    "from tensorflow import *\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.losses import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as SK\n",
    "from sklearn.metrics import *\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import io\n",
    "from functions.GPU import *\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf80ac7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Import train, validation and test sets\n",
    "\n",
    "training = r'C:\\Users\\meryc\\OneDrive\\Desktop\\Trabalhos\\COVID\\COVID_classification_base\\train_fold_4.csv'\n",
    "validation = r'C:\\Users\\meryc\\OneDrive\\Desktop\\Trabalhos\\COVID\\COVID_classification_base\\valid_fold_4.csv'\n",
    "test = r'C:\\Users\\meryc\\OneDrive\\Desktop\\Trabalhos\\COVID\\COVID_classification_base\\test_fold_4.csv'\n",
    "\n",
    "\n",
    "train_dataset = pd.read_csv(training, delimiter=',', low_memory=False)\n",
    "val_dataset = pd.read_csv(validation, delimiter=',', low_memory=False)\n",
    "test_dataset = pd.read_csv(test, delimiter=',', low_memory=False)\n",
    "\n",
    "\n",
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f97fed3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Create y_train, y_test and y_test sets\n",
    "\n",
    "task_start = 2\n",
    "tasks = 1\n",
    "task_index = tasks + 2\n",
    "#print('tasks: %s' % tasks)\n",
    "\n",
    "# load training dataset\n",
    "train_dataset = pd.read_csv(training, delimiter=',', low_memory=False)\n",
    "y_train = np.array(train_dataset.iloc[:,2:task_index].values)\n",
    "print(f\"loaded y_train data: {y_train.shape}\")\n",
    "\n",
    "# load validation dataset\n",
    "val_dataset = pd.read_csv(validation, delimiter=',', low_memory=False)\n",
    "y_val = np.array(val_dataset.iloc[:,2:task_index].values)\n",
    "print(f\"loaded y_val data: {y_val.shape}\")\n",
    "\n",
    "# load test dataset\n",
    "test_dataset = pd.read_csv(test, delimiter=',', low_memory=False)\n",
    "y_test = np.array(test_dataset.iloc[:,2:task_index].values)\n",
    "print(f\"loaded y_test data: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc355297",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate ECFP (defaut) fingerprints using RDKit\n",
    "\n",
    "from functions.fingerprints import *\n",
    "\n",
    "train_smiles=train_dataset[\"SMILES\"].values\n",
    "val_smiles=val_dataset[\"SMILES\"].values\n",
    "test_smiles=test_dataset[\"SMILES\"].values\n",
    "X_train = assing_fp(train_smiles,FP_SIZE,RADIUS)\n",
    "X_val = assing_fp(val_smiles,FP_SIZE,RADIUS)\n",
    "X_test = assing_fp(test_smiles,FP_SIZE,RADIUS)\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282c103b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# custom loss function for missing values in input data (i.e. target labels or values)\n",
    "\n",
    "from functions.utils import *\n",
    "\n",
    "\n",
    "# parameters for train network\n",
    "\n",
    "bit_vector = X_train.shape[1]\n",
    "\n",
    "def create_model():\n",
    "    return T.keras.models.Sequential([\n",
    "            T.keras.layers.Dense(10,input_dim=bit_vector, activation='LeakyReLU',kernel_regularizer= T.keras.regularizers.L1(0.002)),\n",
    "            T.keras.layers.Dropout(0.2),\n",
    "            T.keras.layers.Dense(5, activation='LeakyReLU', kernel_regularizer=T.keras.regularizers.L1(0.002)),\n",
    "            T.keras.layers.Dropout(0.2),\n",
    "            T.keras.layers.Dense(3, activation='LeakyReLU', kernel_regularizer=T.keras.regularizers.L1(0.005)),\n",
    "            T.keras.layers.Dropout(0.2),\n",
    "            T.keras.layers.Dense(1, activation='sigmoid'),\n",
    "            ])\n",
    "\n",
    "\n",
    "accuracy = T.keras.metrics.Accuracy()\n",
    "optimizer = Nadam(learning_rate=1e-3)\n",
    "lr_metric = get_lr_metric(optimizer)\n",
    "\n",
    "model = create_model()\n",
    "model.compile(loss = classification_loss(BinaryCrossentropy), metrics = [lr_metric])    \n",
    "model.summary()\n",
    "T.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c204f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Early stopping parameters\n",
    "\n",
    "callbacks_list = [\n",
    "    ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, min_lr=0.00000001, verbose=1, mode='auto',cooldown=0),\n",
    "    ModelCheckpoint(filepath=\"./models/test_model4.h5\", monitor='val_loss', save_best_only=True, verbose=1, mode='auto'),\n",
    "    EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5, mode='min', verbose=1)]\n",
    "\n",
    "\n",
    "# parameters for train network\n",
    "\n",
    "epochs=2000\n",
    "batch_size=24\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=(callbacks_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab0eb2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Plot model history\n",
    "\n",
    "hist = history.history\n",
    "\n",
    "plt.figure(figsize=(13, 9))\n",
    "\n",
    "\n",
    "for label in ['val_loss','loss']:\n",
    "    plt.subplot(221)\n",
    "    plt.plot(hist[label], label = label)\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"loss\")\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot( hist['lr'],hist['val_loss']  )\n",
    "plt.legend()\n",
    "plt.xlabel(\"lr\")\n",
    "plt.ylabel(\"val_loss\")\n",
    "    \n",
    "plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25,\n",
    "                    wspace=0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86915614",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Statistical characteristics of tasks\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "prediction_train = model.predict(X_train)\n",
    "prediction_train = np.where(prediction_train > threshold, 1.0,0.0)\n",
    "prediction_val = model.predict(X_val)\n",
    "prediction_val = np.where(prediction_val > threshold, 1.0,0.0)\n",
    "prediction_test = model.predict(X_test)\n",
    "prediction_test = np.where(prediction_test > threshold, 1.0,0.0)\n",
    "\n",
    "\n",
    "for index1 in range(prediction_train.shape[1]):\n",
    "    \n",
    "    a = pd.DataFrame(y_train[:,index1],prediction_train[:,index1]) \n",
    "    a['y'] = a.index\n",
    "    b = a.dropna()\n",
    "    confusion = SK.confusion_matrix(b[\"y\"], b[0])\n",
    "    #[row, column]\n",
    "    TP = confusion[1, 1]\n",
    "    TN = confusion[0, 0]\n",
    "    FP = confusion[0, 1]\n",
    "    FN = confusion[1, 0]\n",
    "\n",
    "    \n",
    "    for index2 in range(prediction_val.shape[1]):\n",
    "        \n",
    "        a_val = pd.DataFrame(y_val[:,index2],prediction_val[:,index2]) \n",
    "        a_val['y'] = a_val.index\n",
    "        b_val = a_val.dropna()\n",
    "        confusion_val = SK.confusion_matrix(b_val[\"y\"], b_val[0])\n",
    "        #[row, column]\n",
    "        TP_val = confusion_val[1, 1]\n",
    "        TN_val = confusion_val[0, 0]\n",
    "        FP_val = confusion_val[0, 1]\n",
    "        FN_val = confusion_val[1, 0]\n",
    "\n",
    "        \n",
    "        for index3 in range(prediction_test.shape[1]):\n",
    "            \n",
    "            a_test = pd.DataFrame(y_test[:,index3],prediction_test[:,index3]) \n",
    "            a_test['y'] = a_test.index\n",
    "            b_test = a_test.dropna()\n",
    "            confusion_test = SK.confusion_matrix(b_test[\"y\"], b_test[0])\n",
    "            #[row, column]\n",
    "            TP_test = confusion_test[1, 1]\n",
    "            TN_test = confusion_test[0, 0]\n",
    "            FP_test = confusion_test[0, 1]\n",
    "            FN_test = confusion_test[1, 0]\n",
    "\n",
    "            \n",
    "            if index1 == index2 and index1 == index3:\n",
    "                \n",
    "                print((\"Results for task {} (training)\").format(index1+1))\n",
    "                print(\"ACC\\t%.2f\" % ((TN+TP)/(TN+TP+FN+FP)))\n",
    "                print(\"MCC\\t%.2f\" % SK.matthews_corrcoef(b[\"y\"], b[0]))\n",
    "                print(\"kappa\\t%.2f\" % SK.cohen_kappa_score(b[\"y\"], b[0]))\n",
    "                print(\"SE\\t%.2f\" % (TP/(TP+FN)))\n",
    "                print(\"SP\\t%.2f\" % (TN/(TN+FP)))\n",
    "                print(\"PPV\\t%.2f\" % (TP/(TP+FP)))\n",
    "                print(\"NPV\\t%.2f\" % (TN/(TN+FN)))\n",
    "                print(\"TPR\\t%.2f\" %(TP/(TP+FN)))\n",
    "                print(\"FPR\\t%.2f\" %(FP/(FP+TN)))\n",
    "                print(\"F1\\t%.2f\" % SK.f1_score(b[\"y\"], b[0]))\n",
    "                \n",
    "                print((\"Results for task {} (validation)\").format(index2+1))\n",
    "                print(\"ACC\\t%.2f\" % ((TN_val+TP_val)/(TN_val+TP_val+FN_val+FP_val)))\n",
    "                print(\"MCC\\t%.2f\" % SK.matthews_corrcoef(b_val[\"y\"], b_val[0]))\n",
    "                print(\"kappa\\t%.2f\" % SK.cohen_kappa_score(b_val[\"y\"], b_val[0]))\n",
    "                print(\"SE\\t%.2f\" % (TP_val/(TP_val+FN_val)))\n",
    "                print(\"SP\\t%.2f\" % (TN_val/(TN_val+FP_val)))\n",
    "                print(\"PPV\\t%.2f\" % (TP_val/(TP_val+FP_val)))\n",
    "                print(\"NPV\\t%.2f\" % (TN_val/(TN_val+FN_val)))\n",
    "                print(\"TPR\\t%.2f\" %(TP_val/(TP_val+FN_val)))\n",
    "                print(\"FPR\\t%.2f\" %(FP_val/(FP_val+TN_val)))\n",
    "                print(\"F1\\t%.2f\" % SK.f1_score(b_val[\"y\"], b_val[0]))\n",
    "                \n",
    "                print((\"Results for task {} (test)\").format(index3+1))\n",
    "                print(\"ACC\\t%.2f\" % ((TN_test+TP_test)/(TN_test+TP_test+FN_test+FP_test)))\n",
    "                print(\"MCC\\t%.2f\" % SK.matthews_corrcoef(b_test[\"y\"], b_test[0]))\n",
    "                print(\"kappa\\t%.2f\" % SK.cohen_kappa_score(b_test[\"y\"], b_test[0]))\n",
    "                print(\"SE\\t%.2f\" % (TP_test/(TP_test+FN_test)))\n",
    "                print(\"SP\\t%.2f\" % (TN_test/(TN_test+FP_test)))\n",
    "                print(\"PPV\\t%.2f\" % (TP_test/(TP_test+FP_test)))\n",
    "                print(\"NPV\\t%.2f\" % (TN_test/(TN_test+FN_test)))\n",
    "                print(\"TPR\\t%.2f\" %(TP_test/(TP_test+FN_test)))\n",
    "                print(\"FPR\\t%.2f\" %(FP_test/(FP_test+TN_test)))\n",
    "                print(\"F1\\t%.2f\" % SK.f1_score(b_test[\"y\"], b_test[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
